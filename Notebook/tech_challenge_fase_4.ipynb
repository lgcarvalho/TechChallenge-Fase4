{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **1IADT - Tech Challenge - Fase 4**\n",
    "**Luiz Carvalho**\n",
    "\n",
    "**Grupo:** 48\n",
    "\n",
    "**GitHub:**\n",
    "\n",
    "**Youtube:**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**O PROBLEMA**\n",
    "\n",
    "O Tech Challenge desta fase será a criação de uma aplicação que utilize análise de vídeo. O seu projeto deve incorporar as técnicas de reconhecimento facial, análise de expressões emocionais em vídeos e detecção de atividades."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A PROPOSTA DO DESAFIO**\n",
    "\n",
    "Você deverá criar uma aplicação a partir do vídeo que se encontra disponível na plataforma do aluno, e que execute as seguintes tarefas:\n",
    "\n",
    "1. **Reconhecimento facial**: Identifique e marque os rostos presentes no vídeo.\n",
    "2. **Análise de expressões emocionais**: Analise as expressões emocionais dos rostos identificados.\n",
    "3. **Detecção de atividades**: Detecte e categorize as atividades sendo realizadas no vídeo.\n",
    "4. **Geração de resumo**: Crie um resumo automático das principais atividades e emoções detectadas no vídeo.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Instalando as bibliotecas**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install opencv-python mediapipe python-docx tqdm deepface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Importando bibliotecas**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "from docx import Document\n",
    "from tqdm import tqdm\n",
    "from deepface import DeepFace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Inicia as variaveis de relatório**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variáveis globais\n",
    "resumo_rostos_detectados = {}\n",
    "resumo_emocoes_detectadas = {}\n",
    "resumo_movimentos_detectados = {}\n",
    "contagem_frames = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Inicia as variaveis de detecção**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregar o classificador Haar Cascade para detecção de faces\n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregar o modelo de detecção de pontos faciais e o modelo de detecção de poses do Mediapipe\n",
    "mp_pose = mp.solutions.pose\n",
    "pose = mp_pose.Pose()\n",
    "mp_drawing = mp.solutions.drawing_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1. Reconhecimento facial**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detecta_rosto(frame):\n",
    "    # Converter a imagem para tons de cinza\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Detectar faces na imagem\n",
    "    faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))\n",
    "    \n",
    "    # Para cada região detectada, tentar gerar um embedding e verificar se é um rosto real \n",
    "    for (x, y, w, h) in faces:\n",
    "        # Recortar a região do rosto\n",
    "        face_region = frame[y:y+h, x:x+w]\n",
    "\n",
    "         # Validar se a região está dentro dos limites do frame\n",
    "        if face_region.size == 0 or x < 0 or y < 0:\n",
    "            resumo_rostos_detectados[\"invalidos\"] += 1\n",
    "            \n",
    "            continue\n",
    "\n",
    "        # Pré-processar o rosto para melhorar a qualidade\n",
    "        face_region = cv2.resize(face_region, (160, 160))  # Redimensionar para a entrada esperada pelo modelo\n",
    "        face_region = cv2.cvtColor(face_region, cv2.COLOR_BGR2RGB)  # Converter para RGB se necessário\n",
    "        \n",
    "        try:\n",
    "            # Gerar o embedding usando DeepFace e o modelo Facenet\n",
    "            embedding = DeepFace.represent(face_region, model_name=\"Facenet\")\n",
    "            \n",
    "             # Adicionar validação com o tamanho do embedding para reduzir falsos positivos\n",
    "            if embedding and len(embedding) > 0:\n",
    "                # Desenhar um retângulo ao redor do rosto e marcar como válido\n",
    "                cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
    "                cv2.putText(frame, 'Valido', (x, y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n",
    "                \n",
    "                if resumo_rostos_detectados.get(\"validos\") is None:\n",
    "                    resumo_rostos_detectados[\"validos\"] = 1\n",
    "                else:\n",
    "                    resumo_rostos_detectados[\"validos\"] += 1\n",
    "            else:\n",
    "                if resumo_rostos_detectados.get(\"invalidos\") is None:\n",
    "                    resumo_rostos_detectados[\"invalidos\"] = 1\n",
    "                else:\n",
    "                    resumo_rostos_detectados[\"invalidos\"] += 1           \n",
    "        except Exception as e:\n",
    "            # Adicionar a contagem de rostos inválidos ao resumo\n",
    "            if resumo_rostos_detectados.get(\"invalidos\") is None:\n",
    "                resumo_rostos_detectados[\"invalidos\"] = 1\n",
    "            else:\n",
    "                resumo_rostos_detectados[\"invalidos\"] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2. Análise de expressões emocionais**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detecta_emocoes(frame):\n",
    "    # Processa o frame para detectar emoções\n",
    "    result = DeepFace.analyze(frame, actions=['emotion'], enforce_detection=False)\n",
    "    \n",
    "    # Loop para cada rosto detectado no frame\n",
    "    for face in result:\n",
    "        # Valida se a confiança da detecção é maior que 90%\n",
    "        if face['face_confidence'] > 0.9:\n",
    "            x, y, w, h = face['region']['x'], face['region']['y'], face['region']['w'], face['region']['h']\n",
    "            dominant_emotion = face['dominant_emotion']\n",
    "            \n",
    "            # Registra a emoçção detectada para o resumo\n",
    "            if dominant_emotion in resumo_emocoes_detectadas:\n",
    "                resumo_emocoes_detectadas[dominant_emotion] += 1\n",
    "            else:\n",
    "                resumo_emocoes_detectadas[dominant_emotion] = 1\n",
    "            \n",
    "            # Desenha um retângulo ao redor do rosto e exibe a emoção detectada\n",
    "            cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
    "            cv2.putText(frame, dominant_emotion, (x, y+h+20), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (255,0,0), 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3. Detecção de atividades**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detectar_poses(frame):\n",
    "    # Converte o quadro para RGB\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Detecta poses\n",
    "    results = pose.process(frame_rgb)\n",
    "    \n",
    "    # Desenha as poses\n",
    "    if results.pose_landmarks:\n",
    "        mp_drawing.draw_landmarks(frame, results.pose_landmarks, mp_pose.POSE_CONNECTIONS)\n",
    "        \n",
    "        # Obter os pontos de referência da pose\n",
    "        landmarks = results.pose_landmarks.landmark\n",
    "        \n",
    "        # Obter coordenadas do ombro e pulso\n",
    "        ombro_esquerdo = landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER.value]\n",
    "        ombro_direito = landmarks[mp_pose.PoseLandmark.RIGHT_SHOULDER.value]\n",
    "        pulso_esquerdo = landmarks[mp_pose.PoseLandmark.LEFT_WRIST.value]\n",
    "        pulso_direito = landmarks[mp_pose.PoseLandmark.RIGHT_WRIST.value]\n",
    "        \n",
    "        # Obter coordenadas do nariz\n",
    "        nariz = landmarks[mp_pose.PoseLandmark.NOSE.value]\n",
    "        \n",
    "        # Valida se o braço esquerdo ou direito está levantado\n",
    "        if pulso_esquerdo.y < ombro_esquerdo.y:\n",
    "            if resumo_movimentos_detectados.get(\"braço_esquerdo_levantado\") is None:\n",
    "                resumo_movimentos_detectados[\"braço_esquerdo_levantado\"] = 1\n",
    "            else:\n",
    "                resumo_movimentos_detectados[\"braço_esquerdo_levantado\"] += 1              \n",
    "        elif pulso_direito.y < ombro_direito.y:\n",
    "            if resumo_movimentos_detectados.get(\"braço_direito_levantado\") is None:\n",
    "                resumo_movimentos_detectados[\"braço_direito_levantado\"] = 1\n",
    "            else:\n",
    "                resumo_movimentos_detectados[\"braço_direito_levantado\"] += 1\n",
    "        # Valida se a mão esquerda ou direita está no rosto\n",
    "        elif pulso_esquerdo.y < nariz.y:\n",
    "            if resumo_movimentos_detectados.get(\"mão_esquerda_no_rosto\") is None:\n",
    "                resumo_movimentos_detectados[\"mão_esquerda_no_rosto\"] = 1\n",
    "            else:\n",
    "                resumo_movimentos_detectados[\"mão_esquerda_no_rosto\"] += 1\n",
    "        elif pulso_direito.y < nariz.y:\n",
    "            if resumo_movimentos_detectados.get(\"mão_direita_no_rosto\") is None:\n",
    "                resumo_movimentos_detectados[\"mão_direita_no_rosto\"] = 1\n",
    "            else:\n",
    "                resumo_movimentos_detectados[\"mão_direita_no_rosto\"] += 1\n",
    "        # Caso nenhum movimento acima seja detectado, considerar como movimento anômalo\n",
    "        else:\n",
    "            if resumo_movimentos_detectados.get(\"movimento_anomalo\") is None:\n",
    "                resumo_movimentos_detectados[\"movimento_anomalo\"] = 1\n",
    "            else:\n",
    "                resumo_movimentos_detectados[\"movimento_anomalo\"] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4. Geração de resumo**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gerar_resumo():\n",
    "    # Cria um documento\n",
    "    doc = Document()\n",
    "    \n",
    "    # Adiciona um título\n",
    "    doc.add_heading('Resumo da Análise de Vídeo', level=1)\n",
    "    \n",
    "    # Adiciona os dados do resumo\n",
    "    doc.add_heading('Contagem de frames', level=2)\n",
    "    doc.add_paragraph(f\"Total de frames: {contagem_frames}\")\n",
    "    \n",
    "    doc.add_heading('Rostos detectados', level=2)\n",
    "    for key, value in resumo_rostos_detectados.items():\n",
    "        doc.add_paragraph(f\"{key}: {value}\")\n",
    "        \n",
    "    doc.add_heading('Emoções detectadas', level=2)\n",
    "    for key, value in resumo_emocoes_detectadas.items():\n",
    "        doc.add_paragraph(f\"{key}: {value}\")\n",
    "        \n",
    "    doc.add_heading('Movimentos detectados', level=2)\n",
    "    for key, value in resumo_movimentos_detectados.items():\n",
    "        doc.add_paragraph(f\"{key}: {value}\")\n",
    "        \n",
    "    # Salva o documento\n",
    "    doc.save(\"resumo_analise_video.docx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **5. Análise do video**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processa_video(caminho_video, caminho_saida):\n",
    "    global contagem_frames\n",
    "    \n",
    "    # Carrega o vídeo\n",
    "    video = cv2.VideoCapture(caminho_video)\n",
    "    \n",
    "    if not video.isOpened():\n",
    "        print(\"Erro ao abrir o vídeo\")\n",
    "        return\n",
    "    else:\n",
    "        print(\"Vídeo carregado com sucesso\")\n",
    "    \n",
    "    # Obter informações do vídeo\n",
    "    width = int(video.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(video.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = video.get(cv2.CAP_PROP_FPS)\n",
    "    total_frames = int(video.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    \n",
    "    # Processar um frame a cada x frames\n",
    "    intervalo_entre_frames = 5\n",
    "    \n",
    "    # Configuração para salvar o vídeo processado\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    out = cv2.VideoWriter(caminho_saida, fourcc, fps, (width, height))\n",
    "    \n",
    "    # Processar o vídeo\n",
    "    for _ in tqdm(range(total_frames), desc=\"Processando vídeo\"):\n",
    "        ret, frame = video.read()\n",
    "        \n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        # Processar o frame a cada intervalo definido\n",
    "        if contagem_frames % intervalo_entre_frames == 0:\n",
    "            # Detecta rostos\n",
    "            detecta_rosto(frame)\n",
    "            \n",
    "            # Detecta emoções\n",
    "            detecta_emocoes(frame)\n",
    "            \n",
    "            # Detecta poses\n",
    "            detectar_poses(frame)\n",
    "        \n",
    "        # Escreve o quadro processado no arquivo de saída\n",
    "        out.write(frame)\n",
    "        \n",
    "        # Adiciona 1 ao contador de frames\n",
    "        contagem_frames += 1\n",
    "    \n",
    "    # Libera os recursos\n",
    "    video.release()\n",
    "    out.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    \n",
    "    print(\"Vídeo processado com sucesso\")\n",
    "    \n",
    "    # Gera o documento de resumo\n",
    "    gerar_resumo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vídeo carregado com sucesso\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processando vídeo:   0%|          | 0/3326 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processando vídeo: 100%|██████████| 3326/3326 [04:59<00:00, 11.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vídeo processado com sucesso\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "caminho_video = '../Video/video_tech_challenge.mp4'\n",
    "caminho_saida = '../Video/video_tech_challenge_output.mp4'\n",
    "processa_video(caminho_video, caminho_saida)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
